{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c01ea4",
   "metadata": {},
   "source": [
    "## Testing (NeuroKit2) library to find ECG peaks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355151a4",
   "metadata": {},
   "source": [
    "### ECG Peaks and it Bounds processing:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb52f56f",
   "metadata": {},
   "source": [
    "### Process ECG pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d40221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neurokit2 as nk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf3390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_crossing(ecg_signal, start, end, treshhold, direction=\"up\"):\n",
    "\n",
    "    if start < 0:\n",
    "        start = 0\n",
    "    if end >= len(ecg_signal):\n",
    "        end = len(ecg_signal) - 1\n",
    "\n",
    "    segment = ecg_signal[start:end+1]\n",
    "\n",
    "    if direction == \"up\":\n",
    "        crossing_indexes = np.where((segment[:-1] < treshhold) & (segment[1:] >= treshhold))[0]\n",
    "\n",
    "    else:\n",
    "        crossing_indexes = np.where((segment[:-1] > treshhold) & (segment[1:] <= treshhold))[0]\n",
    "\n",
    "    if len(crossing_indexes) > 0:\n",
    "        i = crossing_indexes[0]\n",
    "        x0, x1 = start + i, start + i + 1\n",
    "        y0, y1 = segment[i], segment[i+1]\n",
    "\n",
    "        if y1 == y0:\n",
    "            return x0\n",
    "\n",
    "        precise_crossing = x0 + (treshhold - y0) * (x1 - x0) / (y1 - y0)\n",
    "        return precise_crossing\n",
    "    return None\n",
    "\n",
    "def plot_qrs_analysis(ecg_signal, search_start, search_end, r_idx, baseline,\n",
    "                      half_amplitude, left_idx, right_idx, qrs_width_ms, beat_number):\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plot_segment = ecg_signal[search_start:search_end]\n",
    "\n",
    "    x_relative = np.arange(len(plot_segment))\n",
    "\n",
    "    plt.plot(x_relative, plot_segment, 'k-', label=\"ECG\")\n",
    "\n",
    "    plt.axhline(baseline, color='gray', linestyle='-', label=\"Baseline (PR)\")\n",
    "    plt.axhline(half_amplitude, color='orange', linestyle='--', label=\"Half R amplitude\")\n",
    "\n",
    "    r_idx_relative = r_idx - search_start\n",
    "    plt.scatter([r_idx_relative], [ecg_signal[r_idx]], color='red', zorder=5,\n",
    "                s=100, label=f\"R-Peak (index {r_idx})\")\n",
    "\n",
    "    if left_idx is not None:\n",
    "        left_idx_relative = left_idx - search_start\n",
    "        plt.axvline(left_idx_relative, color='blue', linestyle=':',\n",
    "                    label=f\"Left Cross (abs: {left_idx:.1f})\")\n",
    "\n",
    "    if right_idx is not None:\n",
    "        right_idx_relative = right_idx - search_start\n",
    "        plt.axvline(right_idx_relative, color='green', linestyle=':',\n",
    "                    label=f\"Right Cross (abs: {right_idx:.1f})\")\n",
    "\n",
    "    plt.title(f\"QRS with at half R Amplitude (Beat {beat_number}): {qrs_width_ms:.1f} ms\")\n",
    "    plt.xlabel(\"Sample (Relative to Segment)\")\n",
    "    plt.ylabel(\"Voltaje (mV)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()\n",
    "\n",
    "def calculate_qrs_width_at_half_r_amplitude(\n",
    "        ecg_signal,\n",
    "        qrs_complex,\n",
    "        sampling_frecuency,\n",
    "        plot_first_beat,\n",
    "        plot_all_beats = False):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    q_peaks = np.array(qrs_complex.get(\"Q_Peaks\", []))\n",
    "    r_peaks = np.array(qrs_complex.get(\"R_Peaks\", []))\n",
    "    s_peaks = np.array(qrs_complex.get(\"S_Peaks\", []))\n",
    "\n",
    "    r_peaks = r_peaks.flatten()\n",
    "    q_peaks = q_peaks.flatten()\n",
    "    s_peaks = s_peaks.flatten()\n",
    "\n",
    "    for i, r_idx in enumerate(r_peaks):\n",
    "        try:\n",
    "            q_peaks_before_r = q_peaks[q_peaks < r_idx]\n",
    "            if q_peaks_before_r.size == 0 :\n",
    "                raise IndexError(\"No Q-peaks found before R-peaks\")\n",
    "\n",
    "            q_idx = int(q_peaks_before_r.max())\n",
    "\n",
    "            s_peaks_after_r = s_peaks[s_peaks > r_idx]\n",
    "            if s_peaks_after_r.size == 0:\n",
    "                raise IndexError(\"No S-peaks found after R-peaks\")\n",
    "\n",
    "            s_idx = int(s_peaks_after_r.min())\n",
    "\n",
    "            pr_window_start = max(0, q_idx - int(0.08 * sampling_frecuency))\n",
    "            pr_window_end = q_idx\n",
    "            if pr_window_end - pr_window_start < 5:\n",
    "                continue\n",
    "            baseline = np.median(ecg_signal[pr_window_start:pr_window_end])\n",
    "\n",
    "            r_amplitude = ecg_signal[r_idx] - baseline\n",
    "            half_amplitude = baseline + r_amplitude / 2\n",
    "\n",
    "            search_start = max(0, int(q_idx) - 10)\n",
    "            search_end = min(len(ecg_signal), int(s_idx) + 10)\n",
    "\n",
    "            left_idx = find_crossing(ecg_signal, search_start, r_idx,\n",
    "                                     half_amplitude, direction=\"up\")\n",
    "            right_idx = find_crossing(ecg_signal, r_idx, search_end,\n",
    "                                     half_amplitude, direction=\"down\")\n",
    "\n",
    "            if left_idx is not None and right_idx is not None:\n",
    "                qrs_samples = right_idx - left_idx\n",
    "                qrs_width_ms = (qrs_samples / sampling_frecuency) * 1000\n",
    "\n",
    "                results.append({\n",
    "                    \"r_peak_index\": r_idx,\n",
    "                    \"qrs_width_ms\": qrs_width_ms,\n",
    "                    \"baseline\": baseline,\n",
    "                    \"half_amplitude\": half_amplitude,\n",
    "                    \"left_crossing_index\": left_idx,\n",
    "                    \"right_crossing_index\": right_idx\n",
    "                })\n",
    "\n",
    "                if plot_all_beats:\n",
    "                    print(f\"Ploting beat {i+1}...\")\n",
    "                    plot_qrs_analysis(ecg_signal, search_start, search_end, r_idx, baseline,\n",
    "                                      half_amplitude, left_idx, right_idx, qrs_width_ms, i+1)\n",
    "                if i == 0 and plot_first_beat:\n",
    "                    print(\"Ploting the first beat...\")\n",
    "                    plot_qrs_analysis(ecg_signal, search_start, search_end, r_idx, baseline,\n",
    "                                      half_amplitude, left_idx, right_idx, qrs_width_ms, i+1)\n",
    "            else:\n",
    "                print(f\"Could not find both crossing points for the beat at index {r_idx}\")\n",
    "\n",
    "        except IndexError:\n",
    "            print(f\"Skiping beat in the index {r_idx} by leak peaks Q/S associated.\")\n",
    "            continue\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "077893b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numpy_to_native(obj):\n",
    "    if isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, dict):\n",
    "        return {key: convert_numpy_to_native(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_to_native(item) for item in obj]\n",
    "    else:\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06ee8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_as_json(to_export):\n",
    "    file_name = f'ecg_processed.json'\n",
    "    ecg_processed = []\n",
    "\n",
    "    content = {}\n",
    "\n",
    "    json_export = convert_numpy_to_native(to_export)\n",
    "\n",
    "    ecg_processed = load_JSON_data(file_name)\n",
    "\n",
    "    ecg_processed.append(json_export)\n",
    "\n",
    "    try:\n",
    "        content[\"ecg\"] = ecg_processed\n",
    "\n",
    "        with open(file_name, 'w', encoding='utf-8') as f:\n",
    "            json.dump(content, f, ensure_ascii=False, indent=4)\n",
    "            print(f'JSON file added successfully')\n",
    "    except Exception as e:\n",
    "        print(f'Error at save JSON: {e}')\n",
    "\n",
    "def load_JSON_data(file_name):\n",
    "    ecg_processed = []\n",
    "    try:\n",
    "        with open(file_name, 'r', encoding='utf-8') as f:\n",
    "            ecg_processed = json.load(f).get(\"ecg\", [])\n",
    "    except json.JSONDecodeError:\n",
    "        print(f'Warning: {file_name} does not contains a valid JSON. A new one will be created')\n",
    "        ecg_processed = []\n",
    "    except Exception as e:\n",
    "        print(f'Error at read existing {file_name} JSON file: {e}')\n",
    "        raise\n",
    "\n",
    "    return ecg_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cea74d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find both crossing points for the beat at index 1356\n",
      "Could not find both crossing points for the beat at index 2579\n",
      "Warning: ecg_processed.json does not contains a valid JSON. A new one will be created\n",
      "JSON file added successfully\n",
      "Skiping beat in the index 368 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 746 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 1186 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 1721 by leak peaks Q/S associated.\n",
      "Could not find both crossing points for the beat at index 4893\n",
      "Skiping beat in the index 368 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 746 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 1186 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 1721 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 363 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 367 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 745 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 1185 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 1719 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 2106 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 2499 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 2898 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 3287 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 3685 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 4046 by leak peaks Q/S associated.\n",
      "JSON file added successfully\n",
      "Skiping beat in the index 191 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 410 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 620 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 188 by leak peaks Q/S associated.\n",
      "Could not find both crossing points for the beat at index 1160\n",
      "Could not find both crossing points for the beat at index 1324\n",
      "Skiping beat in the index 192 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 411 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 621 by leak peaks Q/S associated.\n",
      "Could not find both crossing points for the beat at index 177\n",
      "Could not find both crossing points for the beat at index 607\n",
      "Could not find both crossing points for the beat at index 794\n",
      "Could not find both crossing points for the beat at index 993\n",
      "Could not find both crossing points for the beat at index 1322\n",
      "Could not find both crossing points for the beat at index 1909\n",
      "Could not find both crossing points for the beat at index 2333\n",
      "Could not find both crossing points for the beat at index 2519\n",
      "Could not find both crossing points for the beat at index 2703\n",
      "Could not find both crossing points for the beat at index 2879\n",
      "Could not find both crossing points for the beat at index 3099\n",
      "Could not find both crossing points for the beat at index 3698\n",
      "Could not find both crossing points for the beat at index 4146\n",
      "Could not find both crossing points for the beat at index 4572\n",
      "Could not find both crossing points for the beat at index 1165\n",
      "Could not find both crossing points for the beat at index 1323\n",
      "Skiping beat in the index 183 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 186 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 406 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 616 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 188 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 407 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 617 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 189 by leak peaks Q/S associated.\n",
      "JSON file added successfully\n",
      "Could not find both crossing points for the beat at index 3045\n",
      "Could not find both crossing points for the beat at index 2078\n",
      "Could not find both crossing points for the beat at index 3280\n",
      "Could not find both crossing points for the beat at index 3046\n",
      "Could not find both crossing points for the beat at index 3046\n",
      "Could not find both crossing points for the beat at index 4012\n",
      "Could not find both crossing points for the beat at index 3048\n",
      "Could not find both crossing points for the beat at index 3050\n",
      "Could not find both crossing points for the beat at index 3056\n",
      "JSON file added successfully\n",
      "Skiping beat in the index 591 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 848 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 590 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 847 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 591 by leak peaks Q/S associated.\n",
      "Could not find both crossing points for the beat at index 3840\n",
      "Could not find both crossing points for the beat at index 574\n",
      "Could not find both crossing points for the beat at index 2165\n",
      "Could not find both crossing points for the beat at index 2709\n",
      "Could not find both crossing points for the beat at index 4814\n",
      "Skiping beat in the index 587 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 844 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 589 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 846 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 589 by leak peaks Q/S associated.\n",
      "Skiping beat in the index 847 by leak peaks Q/S associated.\n",
      "JSON file added successfully\n"
     ]
    }
   ],
   "source": [
    "column_names = ['lead_1', 'lead_2', 'lead_3', 'lead_4', 'lead_5', 'lead_6',\n",
    "                'lead_7', 'lead_8', 'lead_9', 'lead_10', 'lead_11', 'lead_12']\n",
    "\n",
    "SAMPLING_RATE = 500\n",
    "\n",
    "schema_report = {\n",
    "    \"file_name\": \"\",\n",
    "    \"deriveds\": []\n",
    "}\n",
    "\n",
    "r_peaks_methods = {}\n",
    "waves_methods = {}\n",
    "\n",
    "folder_path = './ECG_BY_HEARTH_DISEASES/SR/'\n",
    "full_path = folder_path\n",
    "\n",
    "csv_list = [\n",
    "    \"MUSE_20180111_160053_89000.csv\",\n",
    "    \"MUSE_20180111_160140_78000.csv\",\n",
    "    \"MUSE_20180111_160159_36000.csv\",\n",
    "    \"MUSE_20180111_160355_88000.csv\",\n",
    "    \"MUSE_20180111_160410_42000.csv\",\n",
    "]\n",
    "\n",
    "for file_name in csv_list:\n",
    "    full_path += file_name\n",
    "\n",
    "    schema_report['file_name'] = file_name\n",
    "\n",
    "    ecg_df = pd.read_csv(full_path, header=0, names=column_names, encoding='utf-8')\n",
    "\n",
    "    for ecg_lead in ecg_df.columns:\n",
    "        derived = {}\n",
    "        samples = {}\n",
    "\n",
    "        derived[\"name\"] = ecg_lead\n",
    "\n",
    "        ecg_signal = list(ecg_df.loc[:, ecg_lead])\n",
    "\n",
    "        signals, info = nk.ecg_process(ecg_signal, sampling_rate=SAMPLING_RATE)\n",
    "\n",
    "        clean_signal = signals[\"ECG_Clean\"].to_numpy()\n",
    "        r_peaks = info[\"ECG_R_Peaks\"]\n",
    "        q_peaks = info[\"ECG_Q_Peaks\"]\n",
    "        s_peaks = info[\"ECG_S_Peaks\"]\n",
    "\n",
    "        qrs_complex = {}\n",
    "        qrs_complex[\"Q_Peaks\"] = q_peaks\n",
    "        qrs_complex[\"R_Peaks\"] = r_peaks\n",
    "        qrs_complex[\"S_Peaks\"] = s_peaks\n",
    "\n",
    "        samples['R-Peaks'] = r_peaks\n",
    "\n",
    "        waves = {}\n",
    "        waves[\"P_Wave\"] = [info[\"ECG_P_Onsets\"], info[\"ECG_P_Peaks\"], info[\"ECG_P_Offsets\"]]\n",
    "        waves[\"Q\"] = q_peaks\n",
    "        waves[\"S\"] = s_peaks\n",
    "\n",
    "        samples['Waves'] = waves\n",
    "\n",
    "        qrs_w_hra = calculate_qrs_width_at_half_r_amplitude(clean_signal,\n",
    "                                                  qrs_complex,\n",
    "                                                  SAMPLING_RATE,\n",
    "                                                  False,\n",
    "                                                  False)\n",
    "\n",
    "        samples['qrs_w_hra'] = qrs_w_hra\n",
    "\n",
    "        derived['record'] = samples\n",
    "\n",
    "        schema_report['deriveds'].append(derived)\n",
    "\n",
    "    export_as_json(schema_report)\n",
    "    full_path = folder_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
